import numpy as np
import sys
import math
from . import Cost_function as cf
from . import GreedyRandomizedConstructionProcedure as build_first_solution
from . import LocalSearchProcedure as local_search
import datetime as dt
import random

"""
    ----------------------
    Algorithm description
    ----------------------
    The Reactive Greedy Randomized Adaptive Search Procedure for semantic Semi-supervised Trajectory 
    Segmentation (RGRASP-SemTS) is an algorithm that segments trajectories combining a limited user 
    labeled dataset with a low number of input parameters and no predefined segmenting criteria.
    
    When using this code, please refer to: 
    
    Junior, Amilcar Soares, Valeria Cesario Times, Chiara Renso, Stan Matwin, and Luc√≠dio AF Cabral. 
    "A semi-supervised approach for the semantic segmentation of trajectories." 
    In 2018 19th IEEE International Conference on Mobile Data Management (MDM), pp. 145-154. IEEE, 2018.    
    
    Original published version: https://ieeexplore.ieee.org/abstract/document/8411271/
    DOI: https://doi.org/10.1109/MDM.2018.00031
    Author version: https://www.researchgate.net/publication/324841537_A_Semi-Supervised_Approach_for_the_Semantic_Segmentation_of_Trajectories
    

"""

"""
    This is the main method of RGRASP-SemTS, and it receives as parameters:
    -----------------
    Input parameters
    -----------------
    (1) seed (int)      
            A seed value for reproducing experiments
    (2) trajectory_data ([[float, ...]...]) 
            A numpy multidimensional matrix with the trajectory data to be segmented.
            It needs to have the following format: 
                [traj_id, x, y, time(string), [traj. features]]
    (3) labeled_dataset. ([[float,...]]) 
            A numpy multidimensional matrix with the labeled trajectory data.
            It needs to have the following format: 
                [traj_id, x, y, time(string), [traj. features], class (string)]
    (5) reactive_proportion (float)
            A reactive proportion value from 0. -- 1. for updating gradually the probabilities
            of the parameters min_time and alpha
    (6) lists_size (float)
            The size of the lists for parameters alpha and min_time
    (7) max_iterations (int)
            The number of max iterations for building and optimizing solutions
    (8) max_value
            The maximal similarity value possible between two trajectory points
    (8) min_time_bounds [min(float), max(float)]
            The min and max value for the parameter min_time in seconds
    (9) segments_feats_list [(string),...] (optional)
            A list with all segment features attributes to be computed 
    -----------------
    Returns
    -----------------
    (1) total_cost (float)
            The total cost of the RGRASP-SemTS function for all traj. segments found
    (2) segment_id_list ([int, ...])
            A list with the segment which every single traj. point was placed
    (3) y_new ([int, ...]) 
            A list with the class with every single traj. point's class
    
"""


def execute(seed, trajectory_data, labeled_dataset, reactive_proportion, lists_size, max_iterations, max_value,
            min_time_bounds, segments_feats_list={'p5', 'p25', 'p50', 'p75', 'p95'}):
    # select unique trajectory ids
    tid_list = np.unique(trajectory_data[:, 0])
    semantic_landmarks_pf, semantic_landmarks_sf, class_name_index, class_proportion = \
        _create_semantic_landmarks(labeled_dataset, segments_feats_list)
    segment_id_index = 1
    segment_id_list = None
    y_new = None
    total_cost = 0.
    for tid in tid_list:
        # slicing the data to get trajectory id (tid), latitude, longitude and time in a numpy array
        trajectory = trajectory_data[trajectory_data[:, 0] == tid]
        trajectory_st = trajectory[:, 1:4]
        trajectory_pf = trajectory[:, 4:].astype(float)

        best_segments, best_solution_cost, best_class_list_lh, class_name_index, best_class_list_ldh, best_class_array = \
            _execute_for_single_trajectory(seed * int(tid), trajectory_st, trajectory_pf, semantic_landmarks_pf,
                                           semantic_landmarks_sf, class_name_index, reactive_proportion, lists_size,
                                           max_iterations,
                                           max_value, min_time_bounds, class_proportion, segments_feats_list)
        total_cost += best_solution_cost
        # TODO too slow, opportunity to paralelize
        for index in range(len(best_class_array)):
            times_to_add = (int(best_segments[index][1]) - int(best_segments[index][0]) + 1)
            arr_sid = np.empty(times_to_add)
            arr_sid.fill(segment_id_index)
            arr_y = np.empty(times_to_add)
            arr_y.fill(best_class_array[index])

            if segment_id_list is None:
                segment_id_list = arr_sid
                y_new = arr_y
            else:
                segment_id_list = np.concatenate((segment_id_list, arr_sid))
                y_new = np.concatenate((y_new, arr_y))

            segment_id_index += 1
    y_new = y_new.astype(int)
    y_pred_cls = []
    for i in y_new:
        # print(i, y_new[i], class_name_index[y_new[i]])
        y_pred_cls.append(class_name_index[i])
    return total_cost, segment_id_list.astype(int), y_pred_cls


def _execute_for_single_trajectory(seed, trajectory_st, trajectory_pf, semantic_landmarks_pf, semantic_landmarks_sf,
                                   class_name_index, reactive_proportion, lists_size, max_iterations, max_value,
                                   min_time_bounds, class_proportion, segments_feats_list):
    # initialize variables constant variables
    random.seed(seed)
    reactive_proportion_val = reactive_proportion * max_iterations

    min_time_list = np.linspace(min_time_bounds[0], min_time_bounds[1], lists_size)
    alpha_list = np.linspace(.1, .6, lists_size)
    min_time_probs = [1. / float(lists_size)] * lists_size
    alpha_probs = [1. / float(lists_size)] * lists_size
    avg_min_time_list = [None] * lists_size
    avg_alpha_list = [None] * lists_size

    best_solution_cost, best_segments, best_class_list = sys.float_info.max, None, None

    # start Reactive GRASP strategy
    for it in range(max_iterations):
        random.seed(seed * it)
        alpha, alpha_index = _weighted_choice(alpha_list, alpha_probs, random)
        random.seed(seed * it * 2)
        min_time, min_time_index = _weighted_choice(min_time_list, min_time_probs, random)
        segments, class_list = build_first_solution.execute(random, trajectory_st, trajectory_pf, min_time, alpha,
                                                            semantic_landmarks_pf, class_proportion)
        optimized_segments, current_cost, current_class_list_lh, detailed_cost, current_class_list_ldh, classes_array = local_search.execute(
            segments, trajectory_st, trajectory_pf, min_time, semantic_landmarks_pf, semantic_landmarks_sf, max_value,
            class_list, segments_feats_list)

        #         swap if best solution so far
        if current_cost < best_solution_cost:
            best_solution_cost = current_cost
            best_segments = optimized_segments
            best_class_list_lh = current_class_list_lh
            best_class_list_ldh = current_class_list_ldh
            best_class_array = classes_array

        #         update average solutions for min_time and alpha lists
        if avg_min_time_list[min_time_index] is None:
            avg_min_time_list[min_time_index] = current_cost
        else:
            avg_min_time_list[min_time_index] = (avg_min_time_list[min_time_index] + current_cost) / 2.

        if avg_alpha_list[alpha_index] is None:
            avg_alpha_list[alpha_index] = current_cost
        else:
            avg_alpha_list[alpha_index] = (avg_alpha_list[alpha_index] + current_cost) / 2.

        #         update weights when reactive proportion is reach
        if (it + 1) % reactive_proportion_val == 0:
            min_time_probs = _update_list_probs(best_solution_cost, avg_min_time_list)
            alpha_probs = _update_list_probs(best_solution_cost, avg_alpha_list)

    return best_segments, best_solution_cost, best_class_list_lh, class_name_index, best_class_list_ldh, best_class_array


"""
    A local method for updating the lists' probabilities of selecting a value. 
    -----------------
    Input parameters
    -----------------
   (1) best_solution_cost (float) 
           The best value of the cost function found
   (2) avg_list [(float),...] 
           A list with all the averages from a list (alpha or min_time)
    -----------------
    Returns
    -----------------
    (1) new_probs [(float),...] 
            A list with the updated new probabilities
"""


def _update_list_probs(best_solution_cost, avg_list):
    all_avg_qi = [0] * len(avg_list)
    for index in range(len(avg_list)):
        if avg_list[index] is not None:
            all_avg_qi[index] = (best_solution_cost / avg_list[index]) ** 10
        else:
            all_avg_qi[index] = best_solution_cost
    total_qi = sum(all_avg_qi)
    new_probs = [all_avg_qi[index] / total_qi for index in range(len(all_avg_qi))]
    return new_probs


"""
    A local method for randomly choosing a value from a list based on its weight.
    -----------------
    Input parameters
    -----------------
    (1) a_list [(float),...] 
            A list with the computed values for 
    (2) weights [(float),...]
            A list with the current weights of the elements of a_list object
    (3) rnd Random 
            The random object to select an element randomly     
    -----------------
    Returns
    -----------------
    (1) element (float)
        The element randomly chosen from the list using the weighted strategy
    (2) index (int)
        The index of the sorted element
"""


def _weighted_choice(a_list, weights, rnd):
    weights = np.array(weights, dtype=np.float64)
    sum_of_weights = weights.sum()
    # standardization:
    np.multiply(weights, 1 / sum_of_weights, weights)
    weights = weights.cumsum()
    x = rnd.random()
    for index in range(len(weights)):
        if x < weights[index]:
            return a_list[index], index


"""
    A local method for retrieving the minimal time difference between two consecutive trajectory points 
    -----------------
    Input parameters
    -----------------
    (1) time_col [(float),...] 
        A list with all timestamps extracted from a trajectory
    -----------------
    Returns
    -----------------
    (1) The minimal time threshold between consecutive traj points
"""


def _get_min_min_time(time_col):
    min_time = (dt.datetime.strptime(time_col[1], '%Y-%m-%d %H:%M:%S') - dt.datetime.strptime(time_col[0],
                                                                                              '%Y-%m-%d %H:%M:%S')).total_seconds()
    for index in range(1, len(time_col) - 1):
        next_time = (dt.datetime.strptime(time_col[index + 1], '%Y-%m-%d %H:%M:%S') - dt.datetime.strptime(
            time_col[index], '%Y-%m-%d %H:%M:%S')).total_seconds()
        if next_time < min_time:
            min_time = next_time
    return min_time


"""
    A local method for creating the semantic landmarks from the labeled dataset
    -----------------
    Input parameters
    -----------------
    (1) labeled_dataset [(float),...] 
        A list with all timestamps extracted from a trajectory
    (2) segments_feats_list [(string),...] 
        A list with all segment features attributes to be computed
    -----------------
    Returns
    -----------------
    (1) semantic_landmarks_pf [(float),...]
            A numpy multidimensional matrix with the semantic landmarks point features
    (2) semantic_landmarks_sf[(float),...] 
            A numpy multidimensional matrix with the semantic landmarks segment features
"""


def _create_semantic_landmarks(labeled_dataset, segments_feats_list):
    classes_col = labeled_dataset[:, -1]
    class_proportion = []
    # deleting the spatio-temporal columns
    tids = labeled_dataset[:, 0]
    tids = tids[:, np.newaxis]
    pfs = labeled_dataset[:, 4:]

    # _labeled_dataset = labeled_dataset[:,4:]

    _labeled_dataset = np.concatenate((tids, pfs), axis=1)
    classes = np.sort(np.unique(classes_col))
    semantic_landmarks_pf, semantic_landmarks_sf = None, None
    class_name_index = []
    for item in classes:
        #       select trajectory points from a single class
        idx_for_class = (_labeled_dataset[:, -1] == item)
        all_tps = _labeled_dataset[idx_for_class]
        all_tps = np.delete(all_tps, -1, axis=1)
        class_name_index.append(item)
        #       compute the point and segment features
        # iterate over segments
        sf_for_tids = None
        for tid in np.unique(all_tps[:, 0]):
            idx_for_tid = (all_tps[:, 0] == tid)
            all_tps_for_tid = all_tps[idx_for_tid]
            all_tps_for_tid = np.delete(all_tps_for_tid, 0, axis=1)
            v = cf._calculate_all_sf(all_tps_for_tid, segments_feats_list)
            if sf_for_tids is None:
                sf_for_tids = np.array([v])
            else:
                sf_for_tids = np.concatenate((sf_for_tids, [v]))

        all_sf = cf._calculate_all_sf(all_tps[:, 1:], segments_feats_list)
        tps_mean = all_tps[:, 1:].mean(0)
        all_sf = sf_for_tids.mean(0)

        if semantic_landmarks_pf is None and semantic_landmarks_sf is None:
            semantic_landmarks_pf = np.array([tps_mean])
            semantic_landmarks_sf = np.array([all_sf])
        else:
            semantic_landmarks_pf = np.concatenate((semantic_landmarks_pf, [tps_mean]))
            semantic_landmarks_sf = np.concatenate((semantic_landmarks_sf, [all_sf]))
        class_proportion.append(float(all_tps.shape[0]) / float(pfs.shape[0]))

    return semantic_landmarks_pf, semantic_landmarks_sf, class_name_index, class_proportion
